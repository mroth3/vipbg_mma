# ==============================================================================
# Parallel Multimodel Averaging For VIPBG Simulated Twins Data Study
# ==============================================================================

# Load required libraries
require(OpenMx)
require(psych)
require(parallel)
options(width=245)

# ==============================================================================
# UTILITY FUNCTIONS
# ==============================================================================

#' Quiet run function from original mma.R script
quiet_run <- function(file_path, log_file = "error_log.txt") {
  lines <- readLines(file_path, warn = FALSE)
  total_lines <- length(lines)
  con <- textConnection(lines)

  # Open log connection
  log_con <- file(log_file, open = "wt")

  on.exit({
    close(con)
    close(log_con)
  })

  current_line <- 0

  repeat {
    expr <- tryCatch(parse(con, n = 1), error = function(e) NULL)

    # Stop if there's nothing left to run
    if (is.null(expr) || length(expr) == 0) {
      break
    }

    # Track percent complete
    attr_lines <- attr(expr, "srcref")
    if (!is.null(attr_lines)) {
      last_line <- max(sapply(attr_lines, function(x) as.integer(x[3])))
      current_line <- last_line
    } else {
      current_line <- min(total_lines, current_line + 1)
    }

    # Execute expression
    withCallingHandlers(
      tryCatch(
        eval(expr, envir = .GlobalEnv),
        error = function(e) {
          writeLines(sprintf("[%s] ERROR at lines up to %d: %s",
                             format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
                             current_line,
                             e$message), log_con)
        }
      ),
      warning = function(w) {
        writeLines(sprintf("[%s] WARNING at lines up to %d: %s",
                           format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
                           current_line,
                           w$message), log_con)
        invokeRestart("muffleWarning")
      }
    )

    # Break if we've reached the last line
    if (current_line >= total_lines) break
  }
}

#' Create list of all files to process
create_file_list <- function(base_dir = "datasets") {

  # Find all data folders (those starting with "data")
  data_folders <- list.dirs(base_dir, recursive = FALSE, full.names = TRUE)
  data_folders <- data_folders[grepl("data[a-z0-9_]+$", basename(data_folders))]

  all_files <- c()
  for (folder in data_folders) {
    # Find all simulated_data CSV files directly in each data folder
    csv_files <- list.files(folder, pattern = "^simulated_data_.*\\.csv$", full.names = TRUE)
    all_files <- c(all_files, csv_files)
  }

  cat(sprintf("Found %d data folders with simulated datasets:\n", length(data_folders)))
  for (folder in data_folders) {
    folder_files <- list.files(folder, pattern = "^simulated_data_.*\\.csv$")
    cat(sprintf("  %s: %d files\n", basename(folder), length(folder_files)))
  }

  return(all_files)
}

#' Process a single dataset file
process_single_dataset <- function(file_path) {

  cat(sprintf("Processing: %s\n", basename(file_path)))

  tryCatch({
    # Load and prepare data (adapted from your original code)
    allVars <- c('zyg','zyg2','sex1','sex2','p1_t1','p1_t2','p2_t1','p2_t2')
    dataTwo <- read.csv(file_path, header=T, na.strings=".", col.names=allVars)

    # Subset data by zygosity
    dataMZm <- subset(dataTwo, zyg==3,)
    dataMZf <- subset(dataTwo, zyg==1,)
    dataDZm <- subset(dataTwo, zyg==4,)
    dataDZf <- subset(dataTwo, zyg==2,)
    dataDZo <- subset(dataTwo, zyg==5,)

    # Reorder data to match model expectations
    dataMZm <- dataMZm[, c("p1_t1", "p2_t1", "p1_t2", "p2_t2")]
    dataMZf <- dataMZf[, c("p1_t1", "p2_t1", "p1_t2", "p2_t2")]
    dataDZm <- dataDZm[, c("p1_t1", "p2_t1", "p1_t2", "p2_t2")]
    dataDZf <- dataDZf[, c("p1_t1", "p2_t1", "p1_t2", "p2_t2")]
    dataDZo <- dataDZo[, c("p1_t1", "p2_t1", "p1_t2", "p2_t2")]

    # Set up the data environment for multimodel_fitFn.R
    assign("dataMZm", dataMZm, envir = .GlobalEnv)
    assign("dataMZf", dataMZf, envir = .GlobalEnv)
    assign("dataDZm", dataDZm, envir = .GlobalEnv)
    assign("dataDZf", dataDZf, envir = .GlobalEnv)
    assign("dataDZo", dataDZo, envir = .GlobalEnv)

    # Use the quiet_run function like in your original script
    quiet_run('models/multimodel_fitFn.R')

    # Check if the required objects exist after running multimodel_fitFn.R
    if (!exists("fitACE5") || !exists("colPra")) {
      stop("Required objects (fitACE5, colPra) not created by multimodel_fitFn.R")
    }

    # Perform model averaging
    mma <- mxModelAverage(reference=c(colPra,"Prop"),
                         models=fitACE5,
                         include="onlyFree",
                         SE=NULL,
                         refAsBlock=FALSE,
                         type="AIC")

    # Create output directory structure based on input file path
    rel_path <- gsub("^datasets/", "", file_path)
    rel_path <- gsub("\\.csv$", "", rel_path)

    # Create directories for each component
    components <- names(mma)  # Use actual component names from mma object

    for (component in components) {
      # Clean component name for directory creation
      clean_component <- gsub("[^A-Za-z0-9_-]", "_", component)
      output_dir <- file.path(clean_component, dirname(rel_path))
      if (!dir.exists(output_dir)) {
        dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
      }

      # Save the component
      output_file <- file.path(output_dir, paste0(basename(rel_path), ".rds"))
      saveRDS(mma[[component]], output_file)
    }

    cat(sprintf("‚úÖ Completed: %s\n", basename(file_path)))

    return(list(success = TRUE, file = file_path))

  }, error = function(e) {
    cat(sprintf("‚ùå Error processing %s: %s\n", basename(file_path), e$message))

    # Write detailed error to log file
    error_log <- "parallel_processing_errors.txt"
    write(paste(Sys.time(), "- Error in", basename(file_path), ":", e$message),
          file = error_log, append = TRUE)

    return(list(success = FALSE, file = file_path, error = e$message))
  })
}

# ==============================================================================
# MAIN PARALLEL PROCESSING FUNCTION
# ==============================================================================

#' Main function to run parallel MMA analysis
run_parallel_mma <- function(n_cores = NULL, base_dir = "datasets") {

  # Determine number of cores
  if (is.null(n_cores)) {
    n_cores <- max(1, detectCores() - 1)  # Use n-1 cores
  }

  cat(sprintf("üöÄ Starting parallel MMA analysis with %d cores\n", n_cores))

  # Get list of all files to process
  cat("üìã Creating file list...\n")
  all_files <- create_file_list(base_dir)

  cat(sprintf("üìä Found %d total files to process\n", length(all_files)))

  if (length(all_files) == 0) {
    cat("‚ùå No files found to process!\n")
    return(NULL)
  }

  # Start timing
  start_time <- Sys.time()

  # Set up parallel cluster
  cl <- makeCluster(n_cores)

  # Load required packages and functions on each worker
  clusterEvalQ(cl, {
    require(OpenMx)
    require(psych)
    source("models/miFunctions.R")
    options(width=245)
  })

  # Export functions to workers
  clusterExport(cl, c("process_single_dataset", "quiet_run"))

  # Process files in parallel
  cat("üîÑ Starting parallel processing...\n")
  results <- parLapply(cl, all_files, process_single_dataset)

  # Stop cluster
  stopCluster(cl)

  # Calculate summary statistics
  end_time <- Sys.time()
  total_time <- as.numeric(difftime(end_time, start_time, units = "mins"))

  # Count successful and failed processing
  successful <- sum(sapply(results, function(x) x$success))
  failed <- length(results) - successful

  cat("\n", rep("=", 50), "\n")
  cat("üéâ PARALLEL PROCESSING COMPLETE!\n")
  cat(sprintf("‚è±Ô∏è  Total time: %.2f minutes\n", total_time))
  cat(sprintf("üìÅ Total files: %d\n", length(all_files)))
  cat(sprintf("‚úÖ Successfully processed: %d\n", successful))
  cat(sprintf("‚ùå Failed: %d\n", failed))
  cat(sprintf("‚ö° Average rate: %.2f files/minute\n", successful / total_time))
  cat(sprintf("üñ•Ô∏è  Used %d cores\n", n_cores))
  cat(rep("=", 50), "\n")

  if (failed > 0) {
    cat("üìù Check 'parallel_processing_errors.txt' for error details\n")
  }

  return(list(
    total_files = length(all_files),
    successful = successful,
    failed = failed,
    processing_time = total_time,
    cores_used = n_cores,
    results = results
  ))
}

# ==============================================================================
# TEST FUNCTION FOR SINGLE FILE
# ==============================================================================

#' Test processing a single file (for debugging)
test_single_file <- function(base_dir = "datasets") {
  all_files <- create_file_list(base_dir)
  if (length(all_files) > 0) {
    cat("Testing with first file:", all_files[1], "\n")
    result <- process_single_dataset(all_files[1])
    return(result)
  } else {
    cat("No files found to test\n")
    return(NULL)
  }
}

# ==============================================================================
# USAGE EXAMPLES
# ==============================================================================

# Test with a single file first:
# test_result <- test_single_file()

# Run with default settings (n-1 cores)
# results <- run_parallel_mma()

# Run with specific number of cores
# results <- run_parallel_mma(n_cores = 95)
